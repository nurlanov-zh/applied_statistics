{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задача 8\n",
    "### Плюснин Павел 578 / plyusnin.pa@phystech.edu\n",
    "Сначала посчитаем BLEU-скоры для всех пар переводчиков. Занесем эти данные в словарь bleus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "transs = {}\n",
    "bleus = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "import glob\n",
    "import re\n",
    "\n",
    "for file_name in glob.glob(\"./*.txt\"):\n",
    "    with open(file_name, encoding=\"utf8\") as file:\n",
    "        lang_from, lang_to, ttype = re.findall(\".*(..)_(..)_(.).*\", file_name)[0]\n",
    "        text = file.read()\n",
    "        text = tokenizer.tokenize(text)\n",
    "        try:\n",
    "            transs[lang_from][lang_to][ttype] = list(map(lambda x: x.lower(), text))\n",
    "        except KeyError:\n",
    "            try:\n",
    "                transs[lang_from][lang_to] = dict()\n",
    "                transs[lang_from][lang_to][ttype] = list(map(lambda x: x.lower(), text))\n",
    "                \n",
    "                bleus[lang_from][lang_to] = dict()\n",
    "            except KeyError:\n",
    "                transs[lang_from] = dict()\n",
    "                transs[lang_from][lang_to] = dict()\n",
    "                transs[lang_from][lang_to][ttype] = list(map(lambda x: x.lower(), text))\n",
    "                \n",
    "                bleus[lang_from] = dict()\n",
    "                bleus[lang_from][lang_to] = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "for from_lang, dictt in transs.items():\n",
    "    for to_lang, dictt in dictt.items():\n",
    "        for ttype, text in dictt.items():\n",
    "            if ttype != \"g\":\n",
    "                bleus[from_lang][to_lang][ttype] = sentence_bleu([transs[from_lang][to_lang][\"g\"]], text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'en': {'de': {'n': 0.1990509245610674,\n",
       "   'p': 0.19103087482685588,\n",
       "   'z': 0.10009487408556153},\n",
       "  'ru': {'n': 0.17524305655225547,\n",
       "   'p': 0.22330156919355987,\n",
       "   'z': 0.14029326439049417}},\n",
       " 'kk': {'ru': {'n': 0.3602675096515449,\n",
       "   'p': 0.2675619204061147,\n",
       "   'z': 0.3606710891026718}}}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bleus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Требуется определить для каких пар языков можно выделить какого-либо лидера среди переводчиков"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Требуется как-то формализовать нашу задачу. Будем считать для каждой пары языков, что $X_1$ - выборка из двух скоров, не являющимися макс. элементами, а $X_2$ - выборка, сотсоящая как раз из нашего лидера:  \n",
    "$$X_1 = (s_{(1)}, s_{(2)}), X_2 = (s_{(3)})$$  \n",
    "$$H_0: \\mathbb{E}X_1 = \\mathbb{E}X_2: \\text{среднее не изменилось}$$\n",
    "$$H_1: \\mathbb{E}X_1 \\neq \\mathbb{E}X_2: \\text{среднее увеличилось}$$\n",
    "Стоит отметить, что гипотезы выше сформулированы неформально, конкретные гипотезы зависят от используемого критерия.  \n",
    "Но с такой формализацией мы терпим фиаско, так как ни один критерий не может справиться с такими малыми выборками размера 1 и 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scipy.stats as st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pairs = {}\n",
    "for from_lang, dictt in bleus.items():\n",
    "    for to_lang, dictt in dictt.items():\n",
    "        pair = from_lang + \"-\" + to_lang\n",
    "        scores = dictt.values()\n",
    "        maxx = max(scores)\n",
    "        pairs[pair] = [[score for score in scores if score != maxx], [maxx]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'en-de': [[0.19103087482685588, 0.10009487408556153], [0.1990509245610674]],\n",
       " 'en-ru': [[0.17524305655225547, 0.14029326439049417], [0.22330156919355987]],\n",
       " 'kk-ru': [[0.3602675096515449, 0.2675619204061147], [0.3606710891026718]]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Критерии не применяются в связи с малостью выборок. Ни о каких поправках на множественность гипотез, мы тут не говорим)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "en-de MannwhitneyuResult(statistic=0.0, pvalue=0.27014568730370997)\n",
      "en-ru MannwhitneyuResult(statistic=0.0, pvalue=0.27014568730370997)\n",
      "kk-ru MannwhitneyuResult(statistic=0.0, pvalue=0.27014568730370997)\n"
     ]
    }
   ],
   "source": [
    "for name, vals in pairs.items():\n",
    "    p_val = st.mannwhitneyu(vals[0], vals[1], alternative= 'less')\n",
    "    print(name, p_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "en-de (0.0, -0.053488050104858714)\n",
      "en-ru (0.0, -0.06553340872218505)\n",
      "kk-ru (0.0, -0.04675637407384198)\n"
     ]
    }
   ],
   "source": [
    "from permute.core import two_sample\n",
    "for name, vals in pairs.items():\n",
    "    p_val = two_sample(vals[0], vals[1],  alternative='less') \n",
    "    print(name, p_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Переформализуем задачу:\n",
    "Будем считать для каждой пары языков, что $X$ - выборка из двух разниц максимального скора и одного из двух не max скора:\n",
    "$$X = (X_{(3)} - X_{(1)}, X_{(3)} - X_{(2)})$$\n",
    "Для полученной выборки будем оценивать, насколько ее среднее отличимо от 0. Стоит заметить, что неформально мы тут проверяем скорее не наличие лидера, а наличие проигравших, но это не важно ;)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "deltas = {}\n",
    "for from_lang, dictt in bleus.items():\n",
    "    for to_lang, dictt in dictt.items():\n",
    "        pair = from_lang + \"-\" + to_lang\n",
    "        scores = dictt.values()\n",
    "        maxx = max(scores)\n",
    "        deltas[pair] = [maxx - score for score in scores if score != maxx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'en-de': [0.008020049734211526, 0.09895605047550587],\n",
       " 'en-ru': [0.048058512641304396, 0.0830083048030657],\n",
       " 'kk-ru': [0.00040357945112690086, 0.09310916869655711]}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deltas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "en-de Ttest_1sampResult(statistic=1.1763888816053816, pvalue=0.4485166697473929)\n",
      "en-ru Ttest_1sampResult(statistic=3.7501458331351905, pvalue=0.1658984718678795)\n",
      "kk-ru Ttest_1sampResult(statistic=1.0087066908136133, pvalue=0.4972406041971509)\n"
     ]
    }
   ],
   "source": [
    "for name, vals in deltas.items():\n",
    "    p_val = st.ttest_1samp(vals, 0)\n",
    "    print(name, p_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "По этому тесту Нулевую гипотезу отвергнуть нельзя. Оценим дисперсию нашей выборки, выведем интервалы $[\\mu - 2\\sigma; \\mu + 2\\sigma]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "en-de (-0.07511487545144958, 0.182090975661167)\n",
      "en-ru (0.016106938644901314, 0.11495987879946878)\n",
      "kk-ru (-0.08434912754483473, 0.17786187569251874)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "for name, vals in deltas.items():\n",
    "    sigma = np.std(vals, ddof=1) #при ddof мы также отвергаем гипотезу для 2ой пары\n",
    "    print(name, (np.mean(vals) - 2 *sigma, np.mean(vals) + 2 *sigma))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ура! для пары en-ru доверительный интервал не включает 0, поэтому по правилу $2\\sigma$ мы можем отвергнуть гипотезу об отсутсвии лидера! (на уровне значимости $\\approx 0.05$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Применим поправки на множественность гипотез. Так как у нас вприницпе единственный кандидат на отвержение, то разницы между методом Бонферонни и методом Холма у нас нет."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Поищем соответствующее количество сигм для соответствующей $\\alpha / 3$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0 -0.02883359722969164\n",
      "2.010204081632653 -0.02774292693709651\n",
      "2.020408163265306 -0.02667440070485255\n",
      "2.0306122448979593 -0.02562767793135839\n",
      "2.0408163265306123 -0.02460242047114874\n",
      "2.0510204081632653 -0.023598292689186947\n",
      "2.061224489795918 -0.02261496151301084\n",
      "2.0714285714285716 -0.02165209648274685\n",
      "2.0816326530612246 -0.020709369799011997\n",
      "2.0918367346938775 -0.0197864563687225\n",
      "2.1020408163265305 -0.018883033848828886\n",
      "2.1122448979591835 -0.017998782687998646\n",
      "2.122448979591837 -0.01713338616626908\n",
      "2.13265306122449 -0.0162865304326914\n",
      "2.142857142857143 -0.015457904540989994\n",
      "2.1530612244897958 -0.014647200483261565\n",
      "2.163265306122449 -0.013854113221738403\n",
      "2.173469387755102 -0.013078340718641498\n",
      "2.183673469387755 -0.012319583964147503\n",
      "2.193877551020408 -0.011577547002499963\n",
      "2.204081632653061 -0.010851936956289217\n",
      "2.2142857142857144 -0.01014246404892932\n",
      "2.2244897959183674 -0.009448841625359469\n",
      "2.2346938775510203 -0.008770786171002739\n",
      "2.2448979591836733 -0.008108017329003844\n",
      "2.2551020408163267 -0.007460257915782916\n",
      "2.2653061224489797 -0.006827233934930951\n",
      "2.2755102040816326 -0.00620867458947821\n",
      "2.2857142857142856 -0.005604312292565913\n",
      "2.295918367346939 -0.005013882676554945\n",
      "2.306122448979592 -0.0044371246005964725\n",
      "2.316326530612245 -0.003873780156702974\n",
      "2.326530612244898 -0.0033235946743465604\n",
      "2.336734693877551 -0.0027863167236183393\n",
      "2.3469387755102042 -0.002261698116979457\n",
      "2.357142857142857 -0.0017494939096371308\n",
      "2.36734693877551 -0.0012494623985790897\n",
      "2.377551020408163 -0.0007613651202920645\n",
      "2.387755102040816 -0.00028496684720718743\n",
      "2.3979591836734695 0.00017996441710472003\n",
      "2.4081632653061225 0.0006336574439517774\n",
      "2.4183673469387754 0.0010763377867207304\n",
      "2.4285714285714284 0.0015082277892271602\n",
      "2.438775510204082 0.0019295465951833547\n",
      "2.4489795918367347 0.002340510158751311\n",
      "2.4591836734693877 0.002741331256149785\n",
      "2.4693877551020407 0.0031322194982835207\n",
      "2.479591836734694 0.0035133813443624685\n",
      "2.489795918367347 0.0038850201164792354\n",
      "2.5 0.004247336015114459\n"
     ]
    }
   ],
   "source": [
    "import scipy.integrate as integrate\n",
    "n_sigmas = np.linspace(2, 2.5)\n",
    "for sigm in n_sigmas:\n",
    "    print(sigm, (0.05/3) - (1- integrate.quad(st.norm.pdf, -sigm, sigm)[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.387755 -0.0002849715534713436\n",
      "2.387963244897959 -0.00027536939568478455\n",
      "2.3881714897959183 -0.0002657720116830374\n",
      "2.3883797346938773 -0.00025617939950911203\n",
      "2.3885879795918368 -0.00024659155720624035\n",
      "2.388796224489796 -0.00023700848281809836\n",
      "2.389004469387755 -0.00022743017438936128\n",
      "2.3892127142857142 -0.00021785662996448227\n",
      "2.3894209591836733 -0.00020828784758913574\n",
      "2.3896292040816327 -0.00019872382530921814\n",
      "2.3898374489795917 -0.00018916456117184718\n",
      "2.3900456938775507 -0.00017961005322314136\n",
      "2.39025393877551 -0.00017006029951199472\n",
      "2.390462183673469 -0.000160515298085636\n",
      "2.3906704285714286 -0.00015097504699384742\n",
      "2.3908786734693876 -0.00014143954428485692\n",
      "2.391086918367347 -0.00013190878800966796\n",
      "2.391295163265306 -0.00012238277621828483\n",
      "2.391503408163265 -0.00011286150696171102\n",
      "2.3917116530612246 -0.00010334497829172715\n",
      "2.3919198979591836 -9.383318826077999e-05\n",
      "2.392128142857143 -8.432613492087224e-05\n",
      "2.392336387755102 -7.482381632589394e-05\n",
      "2.392544632653061 -6.53262305299572e-05\n",
      "2.3927528775510205 -5.583337558639698e-05\n",
      "2.3929611224489795 -4.63452495509907e-05\n",
      "2.393169367346939 -3.686185047929377e-05\n",
      "2.393377612244898 -2.738317642719465e-05\n",
      "2.393585857142857 -1.7909225451136906e-05\n",
      "2.3937941020408164 -8.439995608896383e-06\n",
      "2.3940023469387754 1.0245150425282346e-06\n",
      "2.394210591836735 1.048430844402884e-05\n",
      "2.394418836734694 1.9939386537274478e-05\n",
      "2.394627081632653 2.938975126249091e-05\n",
      "2.3948353265306124 3.883540455979287e-05\n",
      "2.3950435714285714 4.8276348368851e-05\n",
      "2.395251816326531 5.771258462822573e-05\n",
      "2.39546006122449 6.714411527647748e-05\n",
      "2.3956683061224493 7.657094225116748e-05\n",
      "2.3958765510204083 8.599306748952387e-05\n",
      "2.3960847959183673 9.541049292833073e-05\n",
      "2.3962930408163268 0.00010482322050381701\n",
      "2.396501285714286 0.00011423125215110144\n",
      "2.3967095306122452 0.00012363458980541378\n",
      "2.3969177755102042 0.00013303323540087356\n",
      "2.3971260204081632 0.00014242719087193337\n",
      "2.3973342653061227 0.0001518164581514915\n",
      "2.3975425102040817 0.00016120103917200215\n",
      "2.397750755102041 0.00017058093586658565\n",
      "2.397959 0.00017995615016591984\n"
     ]
    }
   ],
   "source": [
    "n_sigmas = np.linspace(2.387755, 2.397959)\n",
    "for sigm in n_sigmas:\n",
    "    print(sigm, (0.05/3) - (1- integrate.quad(st.norm.pdf, -sigm, sigm)[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "en-de (-0.10044965178604233, 0.20742575199575974)\n",
      "en-ru (0.006369924039676414, 0.12469689340469368)\n",
      "kk-ru (-0.11017691136371405, 0.20368965951139806)\n"
     ]
    }
   ],
   "source": [
    "for name, vals in deltas.items():\n",
    "    sigma = np.std(vals, ddof=1) \n",
    "    print(name, (np.mean(vals) - 2.394 *sigma, np.mean(vals) + 2.394 *sigma))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы все так же отвергаем вторую гипотезу даже с поправкой! Таким образом для $en-ru$ мы действительно отвергаем гипотезу об отсутсвии проигравших, т.е лидера **можно выделить**!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Стоит отметить, что мы предположили нормальность распредления наших дельт, а также в связи с малостью выборок наши оценки все же не очень надежны"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UPDATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "transs = {}\n",
    "bleus = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "import glob\n",
    "import re\n",
    "\n",
    "for file_name in glob.glob(\"./*.txt\"):\n",
    "    with open(file_name, encoding=\"utf8\") as file:\n",
    "        lang_from, lang_to, ttype = re.findall(\".*(..)_(..)_(.).*\", file_name)[0]\n",
    "        \n",
    "        try:\n",
    "            transs[lang_from][lang_to][ttype]\n",
    "        except KeyError:\n",
    "            try:\n",
    "                transs[lang_from][lang_to][ttype] = list()\n",
    "            except KeyError:\n",
    "                try:\n",
    "                    transs[lang_from][lang_to] = dict()\n",
    "                    transs[lang_from][lang_to][ttype] = list()\n",
    "                    \n",
    "                    bleus[lang_from][lang_to] = dict()\n",
    "                except KeyError:\n",
    "                    transs[lang_from] = dict()\n",
    "                    transs[lang_from][lang_to] = dict()\n",
    "                    transs[lang_from][lang_to][ttype] = list()\n",
    "                    \n",
    "                    bleus[lang_from] = dict()\n",
    "                    bleus[lang_from][lang_to] = dict()\n",
    "            \n",
    "        for text in file:\n",
    "            text = tokenizer.tokenize(text)\n",
    "            transs[lang_from][lang_to][ttype].append(list(map(lambda x: x.lower(), text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\plyus\\AppData\\Local\\conda\\conda\\envs\\py36\\lib\\site-packages\\nltk\\translate\\bleu_score.py:490: UserWarning: \n",
      "Corpus/Sentence contains 0 counts of 4-gram overlaps.\n",
      "BLEU scores might be undesirable; use SmoothingFunction().\n",
      "  warnings.warn(_msg)\n",
      "C:\\Users\\plyus\\AppData\\Local\\conda\\conda\\envs\\py36\\lib\\site-packages\\nltk\\translate\\bleu_score.py:490: UserWarning: \n",
      "Corpus/Sentence contains 0 counts of 3-gram overlaps.\n",
      "BLEU scores might be undesirable; use SmoothingFunction().\n",
      "  warnings.warn(_msg)\n",
      "C:\\Users\\plyus\\AppData\\Local\\conda\\conda\\envs\\py36\\lib\\site-packages\\nltk\\translate\\bleu_score.py:490: UserWarning: \n",
      "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
      "BLEU scores might be undesirable; use SmoothingFunction().\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "for from_lang, dictt in transs.items():\n",
    "    for to_lang, dictt in dictt.items():\n",
    "        for ttype, text in dictt.items():\n",
    "            if ttype != \"g\":\n",
    "                bleus[from_lang][to_lang][ttype] = list()\n",
    "                for index, sentence in enumerate(text):\n",
    "                    bleus[from_lang][to_lang][ttype].append(sentence_bleu([transs[from_lang][to_lang][\"g\"][index]], sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'en': {'de': {'n': [0.4293663233009672,\n",
       "    0.20591724619179663,\n",
       "    0.4734490597792576,\n",
       "    0.2812630724452771,\n",
       "    0.2646625485422896,\n",
       "    0.13120966411271084,\n",
       "    0.19940445989088912,\n",
       "    0.5623413251903491,\n",
       "    0.37239098949398236,\n",
       "    0.34295479618200586],\n",
       "   'p': [0.31007120066002053,\n",
       "    0.16830297866366878,\n",
       "    0.5196599018161566,\n",
       "    0.3800427070295605,\n",
       "    0.5814307369682193,\n",
       "    0.1318331306548015,\n",
       "    0.170595737016168,\n",
       "    0.2559142512628947,\n",
       "    0.6076795808137692,\n",
       "    0.22878386498145054],\n",
       "   'z': [0.17694975149532557,\n",
       "    0.5755901146501323,\n",
       "    0.23819486101149287,\n",
       "    0.36209781491447873,\n",
       "    0.7162326270441588,\n",
       "    0.1312155138646134,\n",
       "    0.4851178848133934,\n",
       "    0.7259795291154771,\n",
       "    0.3508439695638686,\n",
       "    0.32865231624539687]},\n",
       "  'ru': {'n': [0.44116293593227063,\n",
       "    0.3148282689155186,\n",
       "    0.3040559696901293,\n",
       "    0.29693621041385787,\n",
       "    0.14440028187544326,\n",
       "    0.32555630133216146,\n",
       "    0.5900468726392808,\n",
       "    0.360056585428503,\n",
       "    0.6334717766551771,\n",
       "    0.668740304976422],\n",
       "   'p': [0.31455601883230705,\n",
       "    0.4217019135502955,\n",
       "    0.42168909138102545,\n",
       "    0.18201355900100438,\n",
       "    0.21711852081087685,\n",
       "    0.37592003642100463,\n",
       "    0.5331675363405771,\n",
       "    0.30826276460621843,\n",
       "    0.6334717766551771,\n",
       "    0.46713797772820004],\n",
       "   'z': [0.5266403878479265,\n",
       "    0.14752569038049876,\n",
       "    0.6559965570884768,\n",
       "    0.1877646789729767,\n",
       "    0.08839374326825923,\n",
       "    0.5444460596606694,\n",
       "    0.5859059370151705,\n",
       "    0.2592170537135687,\n",
       "    0.7197629007461867,\n",
       "    0.29759282342490984]}},\n",
       " 'kk': {'ru': {'n': [0.34107725495137897,\n",
       "    0.668740304976422,\n",
       "    1.0,\n",
       "    0.45966135761245924,\n",
       "    0.7730551756939454,\n",
       "    0.8824969025845955,\n",
       "    0.5555238068023582,\n",
       "    0.2723929121412353,\n",
       "    0.1933313735327453,\n",
       "    0.32516933459899267],\n",
       "   'p': [0.5494128986804837,\n",
       "    0.668740304976422,\n",
       "    0.43167001068522526,\n",
       "    0.5993954153807813,\n",
       "    0.5095231471606585,\n",
       "    0.7364816905995099,\n",
       "    1.0,\n",
       "    0.24219535355065,\n",
       "    0.4047783030643028,\n",
       "    0.22423870508323301],\n",
       "   'z': [0.7598356856515925,\n",
       "    0.6303647413359293,\n",
       "    1.0,\n",
       "    1.0,\n",
       "    0.7071067811865476,\n",
       "    1.0,\n",
       "    1.0,\n",
       "    0.23689513583146024,\n",
       "    0.20584677402387117,\n",
       "    0.4523474722873315]}}}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bleus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scipy.stats as st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from permute.core import one_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.descriptivestats import sign_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\plyus\\AppData\\Local\\conda\\conda\\envs\\py36\\lib\\site-packages\\scipy\\stats\\morestats.py:2385: UserWarning: Warning: sample size too small for normal approximation.\n",
      "  warnings.warn(\"Warning: sample size too small for normal approximation.\")\n"
     ]
    }
   ],
   "source": [
    "deltas = {}\n",
    "combos = ((0,1), (1,0), (1,2), (2,1), (0,2), (2,0))\n",
    "for from_lang, dictt in bleus.items():\n",
    "    for to_lang, dictt in dictt.items():\n",
    "        pair = from_lang + \"-\" + to_lang\n",
    "        scores = list(dictt.values())\n",
    "        p_vals_wilcoxon = []\n",
    "        p_vals_permute = []\n",
    "        p_vals_sign = []\n",
    "        p_vals_ttest = []\n",
    "        for index, combo in enumerate(combos):\n",
    "            p_vals_wilcoxon.append(st.wilcoxon(scores[combo[0]], scores[combo[1]]))\n",
    "            p_vals_permute.append(two_sample(np.array(scores[combo[0]]), np.array(scores[combo[1]]), alternative=\"less\"))\n",
    "            p_vals_sign.append(sign_test(np.array(scores[combo[0]]) - np.array(scores[combo[1]])))\n",
    "            p_vals_ttest.append(st.ttest_rel(np.array(scores[combo[0]]), np.array(scores[combo[1]])))\n",
    "            \n",
    "            #мы проверяем по 3 пары для 3 пар языков\n",
    "            if (p_vals_wilcoxon[index][1] < (0.1 / (3*3))):\n",
    "                print(combo, \"declined by Wilcoxon for\", pair)\n",
    "            if (p_vals_permute[index][0] < 0.1 / (3*3)): \n",
    "                print(combo, \"declined by PermuteTest for\", pair)\n",
    "            if (p_vals_sign[index][1] < 0.1 / (3*3)): \n",
    "                print(combo, \"declined by SignTest for\", pair)\n",
    "            if (p_vals_ttest[index][1] < 0.1 / (3*3)): \n",
    "                print(combo, \"declined by ttest for\", pair)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как мы видим, даже на уровне значимости 0.1 с поправкой Бонферонни на 9 гипотез (что среди 3 пар языков есть лидирующий переводчик в 3 парах переводчиков) нельзя выделить лидера. Итоговый ответ смотри выше, до UPDATE блока"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
