{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание 8\n",
    "Даны результаты работы трех машинных переводчиков (N,P,Z) на небольших выборках переводов с английского на немецкий, с английского на русский и с казахского на русский (https://github.com/Intelligent-Systems-Phystech/psad-19/tree/master/labs/lab1/lab1_data/mt)\n",
    "Стандартная оценка качества перевода производится с использованием специальной метрики BLEU.\n",
    "\n",
    "\n",
    "Требуется определить для каких пар языков можно выделить какого-либо лидера среди переводчиков. Для BLEU учитывать только \n",
    "слова, регистр не учитывать.\n",
    "\n",
    "## Формат данных\n",
    "Названиие файлов имеет формат lang1_lang2_translation.txt:\n",
    " \n",
    " * lang_1, lang_2 --- языки (перевод с lang_1 на lang_2)\n",
    " * translation - какой переводчик использовался. gold - эталонный вариант, с которым сравнивается перевод от систем машинного перевода.\n",
    "\n",
    "## Ссылки\n",
    "* https://en.wikipedia.org/wiki/BLEU\n",
    "* https://www.nltk.org/api/nltk.translate.html#module-nltk.translate.bleu_score\n",
    "* https://stackoverflow.com/questions/15547409/how-to-get-rid-of-punctuation-using-nltk-tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Импорт основных библиотек"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math as m\n",
    "import matplotlib.pylab as plt\n",
    "import pandas as pd\n",
    "import scipy.stats as st\n",
    "import itertools\n",
    "import math\n",
    "import nltk\n",
    "import glob\n",
    "import nltk.translate.bleu_score as score\n",
    "from statsmodels.stats.descriptivestats import sign_test\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from permute.core import two_sample\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Часть 1. Подготовка данных для статистического анализа."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сначала разберёмся как пользоваться функцией $sentence\\_bleu$ из модуля $nltk.translate.bleu\\_score$. Источник информации: https://machinelearningmastery.com/calculate-bleu-score-for-text-python/. Функция принимает на вход два списка - список справочных переводов предложений list(list(str)) ($reference$), а также предложение-кандидат на перевод list(str) ($candidate$). Функция $sentence\\_bleu$ вычисляет метрику-BLEU между $candidate$ и $reference$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7506238537503395\n"
     ]
    }
   ],
   "source": [
    "reference = [['the', 'quick', 'brown', 'fox', 'jumped', 'over', 'the', 'lazy', 'dog']]\n",
    "candidate = ['the', 'fast', 'brown', 'fox', 'jumped', 'over', 'the', 'lazy', 'dog']\n",
    "bleu = score.sentence_bleu(reference, candidate)\n",
    "print(bleu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Будем пользоваться классом $RegexpTokenizer$, который предоставляет библиотека $nltk$, для того, чтобы можно было с помощью регулярных выражений по предложению получать список слов, состоящих только из буквенных и цифровых символов, и не учитывать знаки пунктуации. Источник информации: ссылка в описании задания - https://stackoverflow.com/questions/15547409/how-to-get-rid-of-punctuation-using-nltk-tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Eighty', 'seven', 'miles', 'to', 'go', 'yet', 'Onward']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "tokenizer.tokenize('Eighty-seven miles to go, yet.  Onward!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим словарь $text\\_transl$, который будет иметь $3$ ключа. Первый ключ - язык $lang\\_from$, с которого переводится текст, второй ключ - язык $lang\\_to$, на который переводится текст, третий - тип переводчика $type\\_transl$. Данными в словаре будет набор предложений, которые даёт переводчик типа $type\\_transl$ при переводе с языка $lang\\_from$ на язык $lang\\_to$. При этом все предложения разбиваются на слова с помощью $tokenizer$, а затем приводятся к нижнему регистру, так как в условии сказано: \"Для BLEU учитывать только слова, регистр не учитывать\". Для считывания файлов из текущей директории воспользуемся модулем $glob$. Также создадим списки для названий языков, с которых происходит перевод, как $arr\\_lang\\_from$; список языков, на которые происходит перевод, как $arr\\_lang\\_to$; типы переводчиков, как $arr\\_type\\_trans$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file_name = en_de_gold.txt was opened\n",
      "lang_from = en\n",
      "lang_to = de\n",
      "type_transl = gold\n",
      "file_name = en_de_gold.txt was closed\n",
      "file_name = en_de_n.txt was opened\n",
      "lang_from = en\n",
      "lang_to = de\n",
      "type_transl = n\n",
      "file_name = en_de_n.txt was closed\n",
      "file_name = en_de_orig.txt was opened\n",
      "lang_from = en\n",
      "lang_to = de\n",
      "type_transl = orig\n",
      "file_name = en_de_orig.txt was closed\n",
      "file_name = en_de_p.txt was opened\n",
      "lang_from = en\n",
      "lang_to = de\n",
      "type_transl = p\n",
      "file_name = en_de_p.txt was closed\n",
      "file_name = en_de_z.txt was opened\n",
      "lang_from = en\n",
      "lang_to = de\n",
      "type_transl = z\n",
      "file_name = en_de_z.txt was closed\n",
      "file_name = en_ru_gold.txt was opened\n",
      "lang_from = en\n",
      "lang_to = ru\n",
      "type_transl = gold\n",
      "file_name = en_ru_gold.txt was closed\n",
      "file_name = en_ru_n.txt was opened\n",
      "lang_from = en\n",
      "lang_to = ru\n",
      "type_transl = n\n",
      "file_name = en_ru_n.txt was closed\n",
      "file_name = en_ru_orig.txt was opened\n",
      "lang_from = en\n",
      "lang_to = ru\n",
      "type_transl = orig\n",
      "file_name = en_ru_orig.txt was closed\n",
      "file_name = en_ru_p.txt was opened\n",
      "lang_from = en\n",
      "lang_to = ru\n",
      "type_transl = p\n",
      "file_name = en_ru_p.txt was closed\n",
      "file_name = en_ru_z.txt was opened\n",
      "lang_from = en\n",
      "lang_to = ru\n",
      "type_transl = z\n",
      "file_name = en_ru_z.txt was closed\n",
      "file_name = kk_ru_gold.txt was opened\n",
      "lang_from = kk\n",
      "lang_to = ru\n",
      "type_transl = gold\n",
      "file_name = kk_ru_gold.txt was closed\n",
      "file_name = kk_ru_n.txt was opened\n",
      "lang_from = kk\n",
      "lang_to = ru\n",
      "type_transl = n\n",
      "file_name = kk_ru_n.txt was closed\n",
      "file_name = kk_ru_orig.txt was opened\n",
      "lang_from = kk\n",
      "lang_to = ru\n",
      "type_transl = orig\n",
      "file_name = kk_ru_orig.txt was closed\n",
      "file_name = kk_ru_p.txt was opened\n",
      "lang_from = kk\n",
      "lang_to = ru\n",
      "type_transl = p\n",
      "file_name = kk_ru_p.txt was closed\n",
      "file_name = kk_ru_z.txt was opened\n",
      "lang_from = kk\n",
      "lang_to = ru\n",
      "type_transl = z\n",
      "file_name = kk_ru_z.txt was closed\n"
     ]
    }
   ],
   "source": [
    "set_lang_from = set()\n",
    "set_lang_to = set()\n",
    "set_type_trans = set()\n",
    "text_transl = {}\n",
    "for file_name in glob.glob(\"*.txt\"):\n",
    "    file_short_name = file_name[:-4]\n",
    "    arr_file_name = file_short_name.split('_')\n",
    "    lang_from = arr_file_name[0]\n",
    "    set_lang_from.add(lang_from)\n",
    "    lang_to = arr_file_name[1]\n",
    "    set_lang_to.add(lang_to)\n",
    "    type_trans = arr_file_name[2]\n",
    "    set_type_trans.add(type_trans)\n",
    "    print(\"file_name = {0} was opened\".format(file_name))\n",
    "    print(\"lang_from = {0}\".format(lang_from))\n",
    "    print(\"lang_to = {0}\".format(lang_to))\n",
    "    print(\"type_transl = {0}\".format(type_trans))\n",
    "    file = open(file_name)\n",
    "    string_arr = []\n",
    "    for string in file:\n",
    "        add_string = tokenizer.tokenize(string)\n",
    "        add_string = [x.lower() for x in add_string]\n",
    "        string_arr.append(add_string)\n",
    "    text_transl.update({(lang_from, lang_to, type_trans): string_arr})\n",
    "    #text = tokenizer.tokenize(text)\n",
    "    #text = [x.lower() for x in text]\n",
    "    #print(text)\n",
    "    #text_transl.update({(lang_from, lang_to, type_trans): text})\n",
    "    file.close()\n",
    "    print(\"file_name = {0} was closed\".format(file_name))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['en', 'kk']\n",
      "['ru', 'de']\n",
      "['p', 'z', 'n']\n"
     ]
    }
   ],
   "source": [
    "arr_lang_from = list(set_lang_from)\n",
    "arr_lang_to = list(set_lang_to)\n",
    "arr_type_trans = list(set_type_trans)\n",
    "arr_type_trans.remove('gold')\n",
    "arr_type_trans.remove('orig')\n",
    "print(arr_lang_from)\n",
    "print(arr_lang_to)\n",
    "print(arr_type_trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text_transl[('en', 'ru', 'n')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_test = text_transl[('en', 'ru', 'n')][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['в',\n",
       " 'последнее',\n",
       " 'время',\n",
       " 'мы',\n",
       " 'работаем',\n",
       " 'по',\n",
       " 'нескольким',\n",
       " 'зарубежным',\n",
       " 'схемам',\n",
       " 'как',\n",
       " 'в',\n",
       " 'государственном',\n",
       " 'так',\n",
       " 'и',\n",
       " 'в',\n",
       " 'частном',\n",
       " 'секторе',\n",
       " 'развиваем',\n",
       " 'сотрудничество',\n",
       " 'не',\n",
       " 'только',\n",
       " 'с',\n",
       " 'чешскими',\n",
       " 'но',\n",
       " 'и',\n",
       " 'с',\n",
       " 'иностранными',\n",
       " 'финансовыми',\n",
       " 'субъектами']"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_orig = text_transl[('en', 'ru', 'gold')][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Наконец-то, по полученным наборам предложений для каждого набора $(arr\\_lang\\_from, arr\\_lang\\_to, arr\\_type\\_trans)$. посчитаем метрику BLEU для каждого предложения из массива предложений $text\\_transl[(arr\\_lang\\_from, arr\\_lang\\_to, arr\\_type\\_trans)]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:523: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:523: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:523: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "dict_bleu = {}\n",
    "for lang_from in arr_lang_from:\n",
    "    for lang_to in arr_lang_to:\n",
    "        for type_trans in arr_type_trans:\n",
    "            if ((lang_from, lang_to, type_trans) in text_transl.keys()):\n",
    "                sentences = text_transl[(lang_from, lang_to, type_trans)]\n",
    "                bleu_arr = []\n",
    "                for i in range(len(sentences)):\n",
    "                    sentence = sentences[i]\n",
    "                    true_sentence = text_transl[(lang_from, lang_to, 'gold')][i]\n",
    "                    bleu = score.sentence_bleu([true_sentence], sentence)\n",
    "                    bleu_arr.append(bleu)\n",
    "                dict_bleu.update({(lang_from, lang_to, type_trans): bleu_arr})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6.5806869883189804e-155,\n",
       " 0.3148282689155186,\n",
       " 4.535506046608914e-155,\n",
       " 4.429302865395896e-155,\n",
       " 0.14440028187544326,\n",
       " 3.976144999995337e-78,\n",
       " 0.5900468726392808,\n",
       " 0.360056585428503,\n",
       " 1.1540791471212467e-231,\n",
       " 1.2183324802375697e-231]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_bleu[('en', 'ru', 'n')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3148282689155186"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score.sentence_bleu([sentence_orig], sentence_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь выведем для каждой пары языков массивы метрики BLEU для каждого переводчика."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['p', 'z', 'n']"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr_type_trans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_de_arr = []\n",
    "for type_trans in arr_type_trans:\n",
    "    en_de_arr.append(dict_bleu[('en', 'de', type_trans)])\n",
    "en_ru_arr = []\n",
    "for type_trans in arr_type_trans:\n",
    "    en_ru_arr.append(dict_bleu[('en', 'ru', type_trans)])\n",
    "kk_ru_arr = []\n",
    "for type_trans in arr_type_trans:\n",
    "    kk_ru_arr.append(dict_bleu[('kk', 'ru', type_trans)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Пара языков английский->немецкий"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>p</th>\n",
       "      <th>z</th>\n",
       "      <th>n</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.100712e-01</td>\n",
       "      <td>1.769498e-01</td>\n",
       "      <td>4.293663e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.055549e-78</td>\n",
       "      <td>1.048628e-231</td>\n",
       "      <td>2.514947e-78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.196599e-01</td>\n",
       "      <td>2.381949e-01</td>\n",
       "      <td>4.734491e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.641609e-78</td>\n",
       "      <td>4.422441e-78</td>\n",
       "      <td>3.435175e-78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8.673017e-155</td>\n",
       "      <td>1.304856e-231</td>\n",
       "      <td>3.232426e-78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.318331e-01</td>\n",
       "      <td>1.312155e-01</td>\n",
       "      <td>1.312097e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.705957e-01</td>\n",
       "      <td>7.236349e-155</td>\n",
       "      <td>1.994045e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3.125580e-78</td>\n",
       "      <td>1.322613e-231</td>\n",
       "      <td>8.388266e-155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9.064563e-155</td>\n",
       "      <td>5.233428e-155</td>\n",
       "      <td>5.554838e-155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2.794226e-78</td>\n",
       "      <td>4.902402e-155</td>\n",
       "      <td>5.115747e-155</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               p              z              n\n",
       "0   3.100712e-01   1.769498e-01   4.293663e-01\n",
       "1   2.055549e-78  1.048628e-231   2.514947e-78\n",
       "2   5.196599e-01   2.381949e-01   4.734491e-01\n",
       "3   4.641609e-78   4.422441e-78   3.435175e-78\n",
       "4  8.673017e-155  1.304856e-231   3.232426e-78\n",
       "5   1.318331e-01   1.312155e-01   1.312097e-01\n",
       "6   1.705957e-01  7.236349e-155   1.994045e-01\n",
       "7   3.125580e-78  1.322613e-231  8.388266e-155\n",
       "8  9.064563e-155  5.233428e-155  5.554838e-155\n",
       "9   2.794226e-78  4.902402e-155  5.115747e-155"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_for_data = {'p': en_de_arr[0], 'z': en_de_arr[1], 'n': en_de_arr[2]}\n",
    "df = pd.DataFrame(data=dict_for_data)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Пара языков английский->русский"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>p</th>\n",
       "      <th>z</th>\n",
       "      <th>n</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.145560e-01</td>\n",
       "      <td>7.855727e-155</td>\n",
       "      <td>6.580687e-155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.217019e-01</td>\n",
       "      <td>1.475257e-01</td>\n",
       "      <td>3.148283e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6.290202e-155</td>\n",
       "      <td>1.195116e-231</td>\n",
       "      <td>4.535506e-155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.223002e-78</td>\n",
       "      <td>2.293243e-78</td>\n",
       "      <td>4.429303e-155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.171185e-01</td>\n",
       "      <td>8.839374e-02</td>\n",
       "      <td>1.444003e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5.607479e-155</td>\n",
       "      <td>9.918892e-232</td>\n",
       "      <td>3.976145e-78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5.331675e-01</td>\n",
       "      <td>5.859059e-01</td>\n",
       "      <td>5.900469e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3.082628e-01</td>\n",
       "      <td>2.592171e-01</td>\n",
       "      <td>3.600566e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.154079e-231</td>\n",
       "      <td>1.311287e-231</td>\n",
       "      <td>1.154079e-231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>6.968148e-155</td>\n",
       "      <td>3.634616e-78</td>\n",
       "      <td>1.218332e-231</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               p              z              n\n",
       "0   3.145560e-01  7.855727e-155  6.580687e-155\n",
       "1   4.217019e-01   1.475257e-01   3.148283e-01\n",
       "2  6.290202e-155  1.195116e-231  4.535506e-155\n",
       "3   2.223002e-78   2.293243e-78  4.429303e-155\n",
       "4   2.171185e-01   8.839374e-02   1.444003e-01\n",
       "5  5.607479e-155  9.918892e-232   3.976145e-78\n",
       "6   5.331675e-01   5.859059e-01   5.900469e-01\n",
       "7   3.082628e-01   2.592171e-01   3.600566e-01\n",
       "8  1.154079e-231  1.311287e-231  1.154079e-231\n",
       "9  6.968148e-155   3.634616e-78  1.218332e-231"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_for_data = {'p': en_ru_arr[0], 'z': en_ru_arr[1], 'n': en_ru_arr[2]}\n",
    "df = pd.DataFrame(data=dict_for_data)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Пара языков казахский->русский"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>p</th>\n",
       "      <th>z</th>\n",
       "      <th>n</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000938e-231</td>\n",
       "      <td>1.384293e-231</td>\n",
       "      <td>5.087741e-155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.218332e-231</td>\n",
       "      <td>1.148419e-231</td>\n",
       "      <td>1.218332e-231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.316700e-01</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.320648e-78</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>5.614022e-78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.600394e-155</td>\n",
       "      <td>1.288230e-231</td>\n",
       "      <td>1.153142e-154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>7.364817e-01</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>8.824969e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>8.286572e-155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2.958026e-78</td>\n",
       "      <td>2.893292e-78</td>\n",
       "      <td>3.326840e-78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>6.037949e-155</td>\n",
       "      <td>2.514086e-78</td>\n",
       "      <td>1.933314e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2.242387e-01</td>\n",
       "      <td>6.747523e-155</td>\n",
       "      <td>3.971419e-78</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               p              z              n\n",
       "0  1.000938e-231  1.384293e-231  5.087741e-155\n",
       "1  1.218332e-231  1.148419e-231  1.218332e-231\n",
       "2   4.316700e-01   1.000000e+00   1.000000e+00\n",
       "3   7.320648e-78   1.000000e+00   5.614022e-78\n",
       "4  7.600394e-155  1.288230e-231  1.153142e-154\n",
       "5   7.364817e-01   1.000000e+00   8.824969e-01\n",
       "6   1.000000e+00   1.000000e+00  8.286572e-155\n",
       "7   2.958026e-78   2.893292e-78   3.326840e-78\n",
       "8  6.037949e-155   2.514086e-78   1.933314e-01\n",
       "9   2.242387e-01  6.747523e-155   3.971419e-78"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_for_data = {'p': kk_ru_arr[0], 'z': kk_ru_arr[1], 'n': kk_ru_arr[2]}\n",
    "df = pd.DataFrame(data=dict_for_data)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Часть 2. Статистическая обработка полученных данных."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сначала попробуем для каждой пары языков и для каждой пары переводчиков применить двухсторонний перестановочный критерий для того, чтобы посмотреть, можно ли в какой-то паре переводчиков на какой-то паре языков выделить лидера."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "en_de for p and z, p_value = 0.3921\n",
      "en_de for p and n, p_value = 0.91402\n",
      "en_de for n and z, p_value = 0.30811999999999995\n"
     ]
    }
   ],
   "source": [
    "print(\"en_de for p and z, p_value = {0}\".format(two_sample(en_de_arr[0], en_de_arr[1], alternative='two-sided')[0]))\n",
    "print(\"en_de for p and n, p_value = {0}\".format(two_sample(en_de_arr[0], en_de_arr[2], alternative='two-sided')[0]))\n",
    "print(\"en_de for n and z, p_value = {0}\".format(two_sample(en_de_arr[1], en_de_arr[2], alternative='two-sided')[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "en_ru for p and z, p_value = 0.42852\n",
      "en_ru for p and n, p_value = 0.68396\n",
      "en_ru for n and z, p_value = 0.7038599999999999\n"
     ]
    }
   ],
   "source": [
    "print(\"en_ru for p and z, p_value = {0}\".format(two_sample(en_ru_arr[0], en_ru_arr[1], alternative='two-sided')[0]))\n",
    "print(\"en_ru for p and n, p_value = {0}\".format(two_sample(en_ru_arr[0], en_ru_arr[2], alternative='two-sided')[0]))\n",
    "print(\"en_ru for n and z, p_value = {0}\".format(two_sample(en_ru_arr[1], en_ru_arr[2], alternative='two-sided')[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kk_ru for p and z, p_value = 0.3886400000000001\n",
      "kk_ru for p and n, p_value = 0.83476\n",
      "kk_ru for n and z, p_value = 0.381\n"
     ]
    }
   ],
   "source": [
    "print(\"kk_ru for p and z, p_value = {0}\".format(two_sample(kk_ru_arr[0], kk_ru_arr[1], alternative='two-sided')[0]))\n",
    "print(\"kk_ru for p and n, p_value = {0}\".format(two_sample(kk_ru_arr[0], kk_ru_arr[2], alternative='two-sided')[0]))\n",
    "print(\"kk_ru for n and z, p_value = {0}\".format(two_sample(kk_ru_arr[1], kk_ru_arr[2], alternative='two-sided')[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как видим, к сожалению, нам не повезло и все полученные $p-value$ значительно больше уровня значимости $\\alpha = 0.05$. Неудивительно, наших данных очень мало - всего $10$ предложений в каждом тексте. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем применить знаковый критерий для массивов вида $\\frac{arr_1}{arr_1 + arr_2}$, где отношение массивов понимается поэлементно, с параметром $\\mu_0 = 0.5$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_de_p = np.array(en_de_arr[0])\n",
    "en_de_z = np.array(en_de_arr[1])\n",
    "en_de_n = np.array(en_de_arr[2])\n",
    "en_ru_p = np.array(en_ru_arr[0])\n",
    "en_ru_z = np.array(en_ru_arr[1])\n",
    "en_ru_n = np.array(en_ru_arr[2])\n",
    "kk_ru_p = np.array(kk_ru_arr[0])\n",
    "kk_ru_z = np.array(kk_ru_arr[1])\n",
    "kk_ru_n = np.array(kk_ru_arr[2])\n",
    "\n",
    "en_de_pz = en_de_p + en_de_z \n",
    "en_de_zn = en_de_z + en_de_n\n",
    "en_de_pn = en_de_p + en_de_n\n",
    "en_ru_pz = en_ru_p + en_ru_z \n",
    "en_ru_zn = en_ru_z + en_ru_n\n",
    "en_ru_pn = en_ru_p + en_ru_n\n",
    "kk_ru_pz = kk_ru_p + kk_ru_z \n",
    "kk_ru_zn = kk_ru_z + kk_ru_n\n",
    "kk_ru_pn = kk_ru_p + kk_ru_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sign-test, en_de, p and z, p_value = 0.001953125\n",
      "sign-test, en_de, n and z, p_value = 0.10937500000000003\n",
      "sign-test, en_de, p and n, p_value = 0.7539062500000002\n"
     ]
    }
   ],
   "source": [
    "print(\"sign-test, en_de, p and z, p_value = {0}\".format(sign_test(en_de_p/en_de_pz, mu0=0.5)[1]))\n",
    "print(\"sign-test, en_de, n and z, p_value = {0}\".format(sign_test(en_de_n/en_de_zn, mu0=0.5)[1]))\n",
    "print(\"sign-test, en_de, p and n, p_value = {0}\".format(sign_test(en_de_p/en_de_pn, mu0=0.5)[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sign-test, en_ru, p and z, p_value = 0.7539062500000002\n",
      "sign-test, en_ru, n and z, p_value = 0.7539062500000002\n",
      "sign-test, en_ru, p and n, p_value = 0.5078125\n"
     ]
    }
   ],
   "source": [
    "print(\"sign-test, en_ru, p and z, p_value = {0}\".format(sign_test(en_ru_p/en_ru_pz, mu0=0.5)[1]))\n",
    "print(\"sign-test, en_ru, n and z, p_value = {0}\".format(sign_test(en_ru_n/en_ru_zn, mu0=0.5)[1]))\n",
    "print(\"sign-test, en_ru, p and n, p_value = {0}\".format(sign_test(en_ru_p/en_ru_pn, mu0=0.5)[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sign-test, kk_ru, p and z, p_value = 1.0\n",
      "sign-test, kk_ru, n and z, p_value = 0.5078125\n",
      "sign-test, kk_ru, p and n, p_value = 0.5078125\n"
     ]
    }
   ],
   "source": [
    "print(\"sign-test, kk_ru, p and z, p_value = {0}\".format(sign_test(kk_ru_p/kk_ru_pz, mu0=0.5)[1]))\n",
    "print(\"sign-test, kk_ru, n and z, p_value = {0}\".format(sign_test(kk_ru_n/kk_ru_zn, mu0=0.5)[1]))\n",
    "print(\"sign-test, kk_ru, p and n, p_value = {0}\".format(sign_test(kk_ru_p/kk_ru_pn, mu0=0.5)[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видим, что в паре переводчиков $p$ и $z$ на паре языков английский-немецкий есть явный лидер на уровне значимости $0.05$. На паре языков английский-немецкий для пары переводчиков $p$ и $n$ мы не можем опровергнуть гипотезу о том, что лидера нет. Если увеличить уровень значимости $\\alpha$ до $0.11$, то в паре переводчиков $n$ и $z$ на этой же паре языков можем утверждать, что лидер есть. Таким образом, на уровне значимости значимости $\\alpha = 0.11$ остаётся два варианта: $p \\approx n < z$ или $p \\approx n > z$.  Попробуем понять какой вариант более вероятен, а именно посмотрим насколько сумма массива $\\frac{en\\_de\\_p}{en\\_de\\_p + en\\_de\\_z}$ отличается от величины $10 \\cdot \\frac{1}{2} = 5$ в случае, если бы переводчики были равны."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.969606167045871"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(en_de_p/en_de_pz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.336156038993996"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(en_de_n/en_de_zn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.54931902794926"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(en_de_p/en_de_pn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На основании этих чисел есть основания полагать, что в паре $p$ и $z$ лидером является $p$ (мы не утверждаем это строго!), поэтому мы склоняемся к варианту $p \\approx n > z$, то есть мы нашли не лидера, а лузера =("
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В этой части кода мы пытались рассмотреть немного другую задачу, где сравнивали переводчики не на какой-то конкретной паре языков, а в целом на всех языках. Для этого мы соединили массивы, соответствующие каждому переводчику, между собой и применил знаковый критерий, аналогично тому, как мы делали это выше. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sign-test, p and z, p_value = 0.06142834573984148\n",
      "sign-test, p and n, p_value = 0.8505540192127226\n",
      "sign-test, n and z, p_value = 0.06142834573984148\n"
     ]
    }
   ],
   "source": [
    "arr_p = en_de_arr[0] + en_ru_arr[0] + kk_ru_arr[0]\n",
    "arr_z = en_de_arr[1] + en_ru_arr[1] + kk_ru_arr[1]\n",
    "arr_n = en_de_arr[2] + en_ru_arr[2] + kk_ru_arr[2]\n",
    "arr_p = np.array(arr_p)\n",
    "arr_z = np.array(arr_z)\n",
    "arr_n = np.array(arr_n)\n",
    "arr_pz = arr_p + arr_z\n",
    "arr_pn = arr_p + arr_n\n",
    "arr_zn = arr_z + arr_n\n",
    "print(\"sign-test, p and z, p_value = {0}\".format(sign_test(arr_p/arr_pz, mu0=0.5)[1]))\n",
    "print(\"sign-test, p and n, p_value = {0}\".format(sign_test(arr_p/arr_pn, mu0=0.5)[1]))\n",
    "print(\"sign-test, n and z, p_value = {0}\".format(sign_test(arr_n/arr_zn, mu0=0.5)[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19.06667818310387"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(arr_p/arr_pz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16.42973262102849"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(arr_p/arr_pn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18.662918314692504"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(arr_n/arr_zn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p_value for p and n = 0.77442\n",
      "p_value for p and z = 0.9010400000000001\n",
      "p_value for z and n = 0.71154\n"
     ]
    }
   ],
   "source": [
    "print(\"p_value for p and n = {0}\".format(two_sample(arr_p, arr_n, alternative='two-sided')[0]))\n",
    "print(\"p_value for p and z = {0}\".format(two_sample(arr_p, arr_z, alternative='two-sided')[0]))\n",
    "print(\"p_value for z and n = {0}\".format(two_sample(arr_z, arr_n, alternative='two-sided')[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
